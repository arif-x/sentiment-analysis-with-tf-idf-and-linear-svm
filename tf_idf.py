# -*- coding: utf-8 -*-
"""tf_idf.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ra7LTLLEOn49rvqJl0MoZtbaR_6X0c1d
"""

import pandas as pd
import re
from sklearn.metrics import accuracy_score
import warnings
warnings.filterwarnings('ignore', 'Solver terminated early.*')

# Import dataset csv
df_dataset= pd.read_excel("mypertaminadata.xlsx")
df_dataset.head()

# Cleansing dataset
df_dataset['cleansing'] = df_dataset['text']
df_dataset['cleansing'] = df_dataset['cleansing'].apply(lambda x: re.sub('(<.*?>)', ' ', x))
df_dataset['cleansing'] = df_dataset['cleansing'].apply(lambda x: re.sub('[,\.!?:()"]', '', x))  
df_dataset['cleansing'] = df_dataset['cleansing'].apply(lambda x: x.strip())
df_dataset['cleansing'] = df_dataset['cleansing'].apply(lambda x: re.sub('[^a-zA-Z"]',' ',x))
# Contoh data pertama yg dicleansing
df_dataset['cleansing'][0]

# Dataset Casefolding
df_dataset['casefolding'] = df_dataset['cleansing']
df_dataset['casefolding'] = df_dataset['casefolding'].apply(lambda x: x.lower())
# Contoh data pertama yg dicasefolding
df_dataset['casefolding'][0]

!pip install Sastrawi

# Steeming dataset

from Sastrawi.Stemmer.StemmerFactory import StemmerFactory

df_dataset['steemed'] = None
for i in range (len(df_dataset['casefolding'])):
    factory = StemmerFactory()
    stemmer = factory.create_stemmer()
    casefolding = stemmer.stem(df_dataset['casefolding'][i])
    df_dataset['steemed'][i] = casefolding
    print(i, end = ' ')

# Contoh data pertama yg disteeming
df_dataset['steemed'][0]

import nltk
nltk.download('punkt')
from nltk.tokenize import word_tokenize
from nltk.tokenize import sent_tokenize

# Proses tokeninzing
data = []
for i in range (len(df_dataset['steemed'])):
    steemed = word_tokenize(df_dataset['steemed'][i])
    data.append(steemed)

# Contoh data pertama yang ditokenizing
data[0]

# Data stopwords
text_file = open("stopwords-id.txt", "r")
lines = text_file.readlines()
stopwords = []
for sub in lines:
    stopwords.append(sub.replace("\n", ""))

# Proses penghapusan stopwords
quote_without_stopwords_arr = []

for i in range(len(df_dataset['steemed'])):
    quote_without_stopwords = []
    for word in data[i]:
        if word.casefold() not in stopwords:
            quote_without_stopwords.append(word)
    quote_without_stopwords_arr.append(quote_without_stopwords)

# Contoh data pertama yang dihapus stopwordsnya
quote_without_stopwords_arr[0]

# Masukkan data yang sudah dihapus stopwordnya & dijadikan list ke dalam dataframe
df_dataset['normalisasi'] = None
df_dataset['tokenizing'] = None
for i in range(len(df_dataset['text'])):
    df_dataset['normalisasi'][i] = ' '.join(quote_without_stopwords_arr[i])
    df_dataset['tokenizing'][i] = quote_without_stopwords_arr[i]

# Fungsi hitung nilai TF
def calc_tf(document):
    tf_dict = {}
    for term in document:
        if term in tf_dict:
            tf_dict[term] += 1
        else:
            tf_dict[term] = 1
            
    for term in tf_dict:
        tf_dict[term] = tf_dict[term] / len(document)
    return tf_dict

df_dataset["tf_dict"] = df_dataset['tokenizing'].apply(calc_tf)

# Fungsi hitung nilai DF
def calc_df(tfDict):
    count_df = {}
    for document in tfDict:
        for term in document:
            if term in count_df:
                count_df[term] += 1
            else:
                count_df[term] = 1
    return count_df

df = calc_df(quote_without_stopwords_arr)

import numpy as np

# Fungsi hitung nilai IDF

n_document = len(df_dataset)

def calc_idf(n_document, df):
    idf_Dict = {}
    for term in df:
        idf_Dict[term] = np.log(n_document / (df[term] + 1))
    return idf_Dict
  
idf = calc_idf(n_document, df)

# Hitung nilai TH-IDF

def calc_tf_idf(tf):
    tf_idf_Dict = {}
    for key in tf:
        tf_idf_Dict[key] = tf[key] * idf[key]
    return tf_idf_Dict

df_dataset["tf_idf_dict"] = df_dataset["tf_dict"].apply(calc_tf_idf)

feature = 25
sorted_df = sorted(df.items(), key=lambda kv: kv[1], reverse=True)[:feature]

unique_term = [item[0] for item in sorted_df]

def calc_tf_idf_vec(tf_idf_dict):
    tf_idf_vector = [0.0] * len(unique_term)

    for i, term in enumerate(unique_term):
        if term in tf_idf_dict:
            tf_idf_vector[i] = tf_idf_dict[term]
    return tf_idf_vector

df_dataset["tf_idf_vec"] = df_dataset["tf_idf_dict"].apply(calc_tf_idf_vec)

print("Sort")
print(sorted_df)
print("print first row matrix tf_idf_vec Series\n")
print(df_dataset["tf_idf_vec"][0])

print("\nmatrix size : ", len(df_dataset["tf_idf_vec"][0]))

df_dataset.head()

# Export ke csv & excel

df_dataset.to_csv('data.csv')
df_dataset.to_excel('data.xlsx')